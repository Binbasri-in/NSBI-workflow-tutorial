{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42143bcf-88aa-4837-9039-3e9b5ea20a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "import copy\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "\n",
    "import mplhep as hep\n",
    "hep.style.use(hep.style.ATLAS)\n",
    "\n",
    "import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa06407e-79df-46fb-9331-555ac157873a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "USE_DASK=False\n",
    "USE_SERVICEX=False\n",
    "USE_SAVED_FEATURES=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7d38a2-db70-49f8-a0cc-bbe01d6164fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['ttbar__nominal', 'ttbar__scaledown', 'ttbar__scaleup', 'ttbar__ME_var', 'ttbar__PS_var', 'single_top_s_chan__nominal', 'single_top_t_chan__nominal', 'single_top_tW__nominal', 'wjets__nominal']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'scaleup', 'nevts': 1278695, 'xsec': 729.84}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fileset = utils.file_input.construct_fileset(\n",
    "    N_FILES_MAX_PER_SAMPLE,\n",
    "    use_xcache=False,\n",
    "    af_name=\"coffea_casa\",  \n",
    "    input_from_eos=False,\n",
    "    xcache_atlas_prefix=None,\n",
    ")\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__scaleup']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3a86a8-6170-4805-bcd9-09ae2a88098a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf6e24ad1914f3c911981850071ec0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "executor = processor.FuturesExecutor(workers=4)\n",
    "\n",
    "run = processor.Runner(\n",
    "    executor=executor,\n",
    "    schema=NanoAODSchema,\n",
    "    savemetrics=True,\n",
    "    metadata_cache={},\n",
    "    chunksize=200000)\n",
    "\n",
    "treename = \"Events\"\n",
    "\n",
    "\n",
    "filemeta = run.preprocess(fileset, treename=treename)  # pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de727a0-e230-46dc-937b-9fdeba014b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labels_dict = {\"ttbar\": 0,\n",
    "#                \"single_top_s_chan\":1,\n",
    "#               \"single_top_t_chan\":2,\n",
    "#               \"single_top_tW\":3,\n",
    "#               \"wjets\":4}\n",
    "\n",
    "# Keep only a subset of the background for illustration\n",
    "labels_dict = {\"ttbar\": 0,\n",
    "              \"single_top_t_chan\":1,\n",
    "              \"wjets\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ac8e99-23b2-42dc-9c55-1eeb602cb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create column accumulator from list\n",
    "def col_accumulator(a):\n",
    "    return processor.column_accumulator(np.array(a))\n",
    "\n",
    "processor_base = processor.ProcessorABC\n",
    "class NSBI_analysis(processor_base):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def process(self, events):\n",
    "        \n",
    "        # Note: This creates new objects, distinct from those in the 'events' object\n",
    "        elecs = events.Electron\n",
    "        muons = events.Muon\n",
    "        jets = events.Jet\n",
    "        \n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        \n",
    "        if process in [\"single_top_s_chan\", \"single_top_tW\"]: \n",
    "            return {}\n",
    "        \n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "        \n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        electron_reqs = (elecs.pt > 10) & (np.abs(elecs.eta) < 2.4) & (elecs.pt < 500)\n",
    "        muon_reqs = ((muons.pt > 10) & (np.abs(muons.eta) < 2.4)) & (muons.pt < 500)\n",
    "        # jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "        jet_reqs = (jets.pt > 15) & (np.abs(jets.eta) < 2.4)\n",
    "\n",
    "        # Only keep objects that pass our requirements\n",
    "        elecs = elecs[electron_reqs]\n",
    "        muons = muons[muon_reqs]\n",
    "        jets = jets[jet_reqs]\n",
    "\n",
    "        ######### Store boolean masks with PackedSelection ##########\n",
    "        selections = PackedSelection(dtype='uint64')\n",
    "        \n",
    "        # Inclusive selection criteria \n",
    "        selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "        \n",
    "        selections.add(\"Inclusive\", selections.all(\"exactly_1l\"))\n",
    "\n",
    "        selection = selections.all(\"Inclusive\")\n",
    "        selected_jets = jets[selection]\n",
    "        selected_elecs = elecs[selection]\n",
    "        selected_muons = muons[selection]\n",
    "        selected_weights = np.ones(len(selected_jets)) * xsec_weight\n",
    "\n",
    "\n",
    "        # grab lepton info\n",
    "        leptons = ak.flatten(ak.concatenate((selected_elecs, selected_muons), axis=1), axis=-1)\n",
    "\n",
    "        # H_T = ak.sum(selected_jets.pt, axis=-1)\n",
    "        pT_lep = leptons.pt\n",
    "        eta_lep = leptons.eta\n",
    "        phi_lep = leptons.phi\n",
    "        \n",
    "        #### calculate features ####\n",
    "        features = np.zeros((len(pT_lep), 3))\n",
    "        \n",
    "        features[:, 0] = pT_lep\n",
    "        features[:, 1] = eta_lep\n",
    "        features[:, 2] = phi_lep\n",
    "\n",
    "\n",
    "\n",
    "        train_labels = np.full_like(pT_lep, labels_dict[process])\n",
    "\n",
    "\n",
    "        output = {\"train_labels\": col_accumulator(train_labels.tolist()),\n",
    "                  \"weights\": col_accumulator(selected_weights.tolist()),\n",
    "                  \"features\": col_accumulator(features.tolist())}\n",
    "\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e4af2-9251-4511-b979-286ff606d740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f57f57ecac4853aa2f4c6c49131dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not USE_SAVED_FEATURES:\n",
    "    \n",
    "    t0 = time.monotonic()\n",
    "    # processing\n",
    "    output, metrics = run(\n",
    "        fileset,\n",
    "        treename,\n",
    "        processor_instance=NSBI_analysis()\n",
    "    )\n",
    "\n",
    "\n",
    "    exec_time = time.monotonic() - t0\n",
    "\n",
    "\n",
    "    print(f\"\\nexecution took {exec_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082773ba-72e6-4197-ac39-3173be5b2b23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_data = \"./cached_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c87686-13e1-4eac-b79d-f214930e7340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_SAVED_FEATURES:\n",
    "    \n",
    "    features = np.load(f\"{saved_data}features.npy\")\n",
    "    train_labels = np.load(f\"{saved_data}train_labels.npy\")\n",
    "    weights = np.load(f\"{saved_data}weights.npy\")\n",
    "    \n",
    "else:\n",
    "\n",
    "    # grab features and labels and convert to np array\n",
    "    features = np.array(output['features'].value)\n",
    "    train_labels = np.array(output['train_labels'].value)\n",
    "    weights = np.array(output['weights'].value)\n",
    "    \n",
    "    np.save(f\"{saved_data}features.npy\", features)\n",
    "    np.save(f\"{saved_data}train_labels.npy\", train_labels)        \n",
    "    np.save(f\"{saved_data}weights.npy\", weights)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd3f99-3422-4210-bf74-70e00c34cff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bins=np.linspace(10.0, 500.0, num=20)\n",
    "hist_lep_pt = {}\n",
    "\n",
    "for key in labels_dict:\n",
    "    \n",
    "    hist_lep_pt[key], _ = np.histogram(features[:,0][train_labels==labels_dict[key]], weights = weights[train_labels==labels_dict[key]], bins=bins)\n",
    "\n",
    "for key in labels_dict:\n",
    "\n",
    "    hep.histplot(hist_lep_pt[key], bins=bins, label=key, density=1)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32066846-28a7-4b58-a502-8488e66d9c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bins=np.linspace(-2.5, 2.5, num=20)\n",
    "hist_lep_eta = {}\n",
    "\n",
    "for key in labels_dict:\n",
    "    \n",
    "    hist_lep_eta[key], _ = np.histogram(features[:,1][train_labels==labels_dict[key]], weights = weights[train_labels==labels_dict[key]], bins=bins)\n",
    "\n",
    "for key in labels_dict:\n",
    "\n",
    "    hep.histplot(hist_lep_eta[key], bins=bins, label=key, density=1)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='lower center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b58e0-23ea-49ba-9025-47ed3b5ae53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bins=np.linspace(-2.5, 2.5, num=20)\n",
    "hist_lep_phi = {}\n",
    "\n",
    "for key in labels_dict:\n",
    "    \n",
    "    hist_lep_phi[key], _ = np.histogram(features[:,2][train_labels==labels_dict[key]], weights = weights[train_labels==labels_dict[key]], bins=bins)\n",
    "\n",
    "for key in labels_dict:\n",
    "\n",
    "    hep.histplot(hist_lep_phi[key], bins=bins, label=key, density=1)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='lower center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93970905-2aeb-442c-90e2-919c4c63ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50506c62-2789-4bd8-bc31-a15c672cc6fa",
   "metadata": {},
   "source": [
    "Training the Preselection NN\n",
    "===\n",
    "\n",
    "Now we train a multi-variate, multi-class classifier just to eliminate phase space with no signal events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d8951-3519-42eb-8dee-1dff96dacef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Nadam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955e9c6-6393-472a-8f68-a793e0b35933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # One-hot encode labels\n",
    "# encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "# train_labels_reshaped = train_labels.reshape(-1, 1)\n",
    "# train_labels_onehot = encoder.fit_transform(train_labels_reshaped)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)  # Fit & transform training data\n",
    "\n",
    "# Split data into training and validation sets (including weights)\n",
    "X_train, X_val, y_train, y_val, weight_train, weight_val = train_test_split(\n",
    "    features_scaled, train_labels, weights, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(features.shape[1],)),  # Input layer\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')  # Output layer for 5 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model with sample weights\n",
    "model.fit(X_train, y_train, sample_weight=weight_train, \n",
    "          validation_data=(X_val, y_val), epochs=5, batch_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978584f-bb7d-48bb-8c7c-a85f96a11368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predictions (softmax outputs)\n",
    "pred_NN = model.predict(features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbbbc0-46c8-4020-9781-e763f953ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "presel_score = np.log(pred_NN[:, labels_dict[\"ttbar\"]]/(np.sum([pred_NN[:, labels_dict[key]] for key in list(labels_dict.keys())[1:]])))\n",
    "\n",
    "print(presel_score.shape)\n",
    "min_pred = np.amin(presel_score)\n",
    "max_pred = np.amax(presel_score)\n",
    "\n",
    "bins = np.linspace(min_pred, max_pred, num=150)\n",
    "\n",
    "hist_NN_output = {}\n",
    "for key in labels_dict: \n",
    "    hist_NN_output[key], _ = np.histogram(presel_score[train_labels==labels_dict[key]], weights = weights[train_labels==labels_dict[key]], bins=bins)\n",
    "\n",
    "\n",
    "for key in labels_dict:  \n",
    "    hep.histplot(hist_NN_output[key], bins=bins, \n",
    "             alpha=0.6, label=key, \n",
    "             density=True, linewidth=2.0)\n",
    "\n",
    "plt.xlabel(\"Preselection Score\", size=18)\n",
    "plt.ylabel(\"Density\", size=18)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46066ad-ac99-45ae-93b8-82fe30d75f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(f'{saved_data}preselection_score.npy', presel_score)\n",
    "np.save(f'{saved_data}preselection_score.npy', presel_score)\n",
    "np.save(f'{saved_data}preselection_score.npy', presel_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524bb683-b07c-4c2b-a342-92a996c0250a",
   "metadata": {},
   "source": [
    "Make a selection cut for performing the NSBI analysis, balancing the number of signal events that go into the signal region (increasing sensitivity) and the feasibility of training accurate and precise NNs over a large phase space (need bigger models and more statistics).\n",
    "\n",
    "As a first cut, we choose a loose preselection and see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5062354-58f1-42ad-b334-a93ae47f2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "preselection_cut = -17.5 # Lets start with a very loose cut\n",
    "\n",
    "for key in labels_dict:  \n",
    "    hep.histplot(hist_NN_output[key], bins=bins, \n",
    "             alpha=0.6, label=key, \n",
    "             density=True, linewidth=2.0)\n",
    "\n",
    "plt.xlabel(\"Preselection Score\", size=18)\n",
    "plt.axvline(preselection_cut, ymax=0.9, linestyle='--', label=f'preselection cut ({preselection_cut})')\n",
    "plt.ylabel(\"Density\", size=18)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afbdd6e-bc01-40d9-8a5d-c0fccc007bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3cfcc8-f078-4156-8046-5c1403260569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded76f9c-bcb5-4821-a105-1a265ae7c091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7562915-7fc5-4f72-b7f6-e9506358b467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
