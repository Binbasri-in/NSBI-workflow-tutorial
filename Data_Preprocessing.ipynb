{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42143bcf-88aa-4837-9039-3e9b5ea20a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "import cloudpickle\n",
    "import correctionlib\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "import copy\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "\n",
    "import utils \n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa06407e-79df-46fb-9331-555ac157873a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_FILES_MAX_PER_SAMPLE = 10\n",
    "USE_DASK=False\n",
    "USE_SERVICEX=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af7d38a2-db70-49f8-a0cc-bbe01d6164fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['ttbar__nominal', 'ttbar__scaledown', 'ttbar__scaleup', 'ttbar__ME_var', 'ttbar__PS_var', 'single_top_s_chan__nominal', 'single_top_t_chan__nominal', 'single_top_tW__nominal', 'wjets__nominal']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'scaleup', 'nevts': 12554872, 'xsec': 729.84}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fileset = utils.file_input.construct_fileset(\n",
    "    N_FILES_MAX_PER_SAMPLE,\n",
    "    use_xcache=False,\n",
    "    af_name=\"coffea_casa\",  \n",
    "    input_from_eos=False,\n",
    "    xcache_atlas_prefix=None,\n",
    ")\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__scaleup']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf3a86a8-6170-4805-bcd9-09ae2a88098a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a8a193a6fa4be695dbf8b16a1962eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "executor = processor.FuturesExecutor(workers=4)\n",
    "\n",
    "run = processor.Runner(\n",
    "    executor=executor,\n",
    "    schema=NanoAODSchema,\n",
    "    savemetrics=True,\n",
    "    metadata_cache={},\n",
    "    chunksize=200000)\n",
    "\n",
    "treename = \"Events\"\n",
    "\n",
    "\n",
    "filemeta = run.preprocess(fileset, treename=treename)  # pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5de727a0-e230-46dc-937b-9fdeba014b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_dict = {\"ttbar\": 1,\n",
    "               \"single_top_s_chan\":2,\n",
    "              \"single_top_t_chan\":3,\n",
    "              \"single_top_tW\":4,\n",
    "              \"wjets\":5}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93ac8e99-23b2-42dc-9c55-1eeb602cb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create column accumulator from list\n",
    "def col_accumulator(a):\n",
    "    return processor.column_accumulator(np.array(a))\n",
    "\n",
    "processor_base = processor.ProcessorABC\n",
    "class NSBI_analysis(processor_base):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def process(self, events):\n",
    "        \n",
    "        # Note: This creates new objects, distinct from those in the 'events' object\n",
    "        elecs = events.Electron\n",
    "        muons = events.Muon\n",
    "        jets = events.Jet\n",
    "        \n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "        \n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "        muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                     (muons.pfRelIso04_all < 0.15))\n",
    "        jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "        # Only keep objects that pass our requirements\n",
    "        elecs = elecs[electron_reqs]\n",
    "        muons = muons[muon_reqs]\n",
    "        jets = jets[jet_reqs]\n",
    "\n",
    "        B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "        ######### Store boolean masks with PackedSelection ##########\n",
    "        selections = PackedSelection(dtype='uint64')\n",
    "        # Basic selection criteria\n",
    "        selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "        selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "        selections.add(\"atleast_1bj\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "\n",
    "        selections.add(\"Inclusive\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_1bj\"))\n",
    "\n",
    "        selection = selections.all(\"Inclusive\")\n",
    "        selected_jets = jets[selection]\n",
    "        selected_elecs = elecs[selection]\n",
    "        selected_muons = muons[selection]\n",
    "        selected_weights = np.ones(len(selected_jets)) * xsec_weight\n",
    "\n",
    "\n",
    "        # calculate number of jets in each event\n",
    "        njet = ak.num(selected_jets).to_numpy()\n",
    "        \n",
    "        permutations_dict = get_permutations_dict(4)\n",
    "\n",
    "        # don't consider every jet for events with high jet multiplicity\n",
    "        njet[njet > max(permutations_dict.keys())] = max(permutations_dict.keys())\n",
    "        # create awkward array of permutation indices\n",
    "        perms = ak.Array([permutations_dict[n] for n in njet])\n",
    "        perm_counts = ak.num(perms)\n",
    "        \n",
    "\n",
    "        # grab lepton info\n",
    "        leptons = ak.flatten(ak.concatenate((selected_elecs, selected_muons), axis=1), axis=-1)\n",
    "\n",
    "        H_T = ak.sum(selected_jets.pt, axis=-1)\n",
    "        pT_lep = leptons.pt\n",
    "        \n",
    "        #### calculate features ####\n",
    "        features = np.zeros((len(pT_lep), 13))\n",
    "        \n",
    "        features[:, 0] = pT_lep\n",
    "        \n",
    "        \n",
    "        # pt of every jet\n",
    "        features[:, 1] = ak.Array(selected_jets[:,0].pt).to_numpy()\n",
    "        features[:, 2] = ak.Array(selected_jets[:,1].pt).to_numpy()\n",
    "        features[:, 3] = ak.Array(selected_jets[:,2].pt).to_numpy()\n",
    "        features[:, 4] = ak.Array(selected_jets[:,3].pt).to_numpy()\n",
    "\n",
    "        # btagCSVV2 of every jet\n",
    "        features[:, 5] = ak.Array(selected_jets[:,0].btagCSVV2).to_numpy()\n",
    "        features[:, 6] = ak.Array(selected_jets[:,1].btagCSVV2).to_numpy()\n",
    "        features[:, 7] = ak.Array(selected_jets[:,2].btagCSVV2).to_numpy()\n",
    "        features[:, 8] = ak.Array(selected_jets[:,3].btagCSVV2).to_numpy()\n",
    "\n",
    "        # quark-gluon likelihood discriminator of every jet\n",
    "        features[:, 9] = ak.Array(selected_jets[:,0].qgl).to_numpy()\n",
    "        features[:, 10] = ak.Array(selected_jets[:,1].qgl).to_numpy()\n",
    "        features[:, 11] = ak.Array(selected_jets[:,2].qgl).to_numpy()\n",
    "        features[:, 12] = ak.Array(selected_jets[:,3].qgl).to_numpy()\n",
    "\n",
    "\n",
    "        train_labels = np.full_like(pT_lep, labels_dict[process])\n",
    "\n",
    "\n",
    "        output = {\"train_labels\": col_accumulator(train_labels.tolist()),\n",
    "                  \"weights\": col_accumulator(selected_weights.tolist()),\n",
    "                  \"features\": col_accumulator(features.tolist()),\n",
    "                    \"H_T\": col_accumulator(H_T.tolist()),\n",
    "                    \"pT_lep\": col_accumulator(pT_lep.tolist()),}\n",
    "\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "215e4af2-9251-4511-b979-286ff606d740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6786da4758354c1baf27871204254b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "execution took 1856.76 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.monotonic()\n",
    "# processing\n",
    "output, metrics = run(\n",
    "    fileset,\n",
    "    treename,\n",
    "    processor_instance=NSBI_analysis()\n",
    ")\n",
    "\n",
    "\n",
    "exec_time = time.monotonic() - t0\n",
    "\n",
    "\n",
    "print(f\"\\nexecution took {exec_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53d14d38-1762-46b3-ac45-30a0b13c8b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab features and labels and convert to np array\n",
    "features = np.array(output['features'].value)\n",
    "train_labels = np.array(output['train_labels'].value)\n",
    "weights = np.array(output['weights'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a1d8951-3519-42eb-8dee-1dff96dacef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b7711-76b5-4938-b9da-bf581ece4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_only=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955e9c6-6393-472a-8f68-a793e0b35933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "51188/51188 [==============================] - 40s 774us/step - loss: 0.0337 - weighted_accuracy: 0.9688 - val_loss: 0.1964 - val_weighted_accuracy: 0.9691\n",
      "Epoch 2/20\n",
      "51188/51188 [==============================] - 39s 764us/step - loss: 0.0329 - weighted_accuracy: 0.9687 - val_loss: 0.1927 - val_weighted_accuracy: 0.9691\n",
      "Epoch 3/20\n",
      "51188/51188 [==============================] - 39s 759us/step - loss: 0.0327 - weighted_accuracy: 0.9687 - val_loss: 0.1931 - val_weighted_accuracy: 0.9691\n",
      "Epoch 4/20\n",
      "51188/51188 [==============================] - 39s 762us/step - loss: 0.0328 - weighted_accuracy: 0.9688 - val_loss: 0.2109 - val_weighted_accuracy: 0.9691\n",
      "Epoch 5/20\n",
      "51188/51188 [==============================] - 39s 763us/step - loss: 0.0328 - weighted_accuracy: 0.9688 - val_loss: 0.1954 - val_weighted_accuracy: 0.9691\n",
      "Epoch 6/20\n",
      "51188/51188 [==============================] - 39s 760us/step - loss: 0.0326 - weighted_accuracy: 0.9688 - val_loss: 0.2021 - val_weighted_accuracy: 0.9691\n",
      "Epoch 7/20\n",
      " 2939/51188 [>.............................] - ETA: 31s - loss: 0.0362 - weighted_accuracy: 0.9682"
     ]
    }
   ],
   "source": [
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "train_labels_reshaped = train_labels.reshape(-1, 1)\n",
    "train_labels_onehot = encoder.fit_transform(train_labels_reshaped)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)  # Fit & transform training data\n",
    "\n",
    "# Split data into training and validation sets (including weights)\n",
    "X_train, X_val, y_train, y_val, weight_train, weight_val = train_test_split(\n",
    "    features_scaled, train_labels_onehot, weights, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(features.shape[1],)),  # Input layer\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')  # Output layer for 5 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"weighted_accuracy\")])\n",
    "\n",
    "# Train the model with sample weights\n",
    "model.fit(X_train, y_train, sample_weight=weight_train, \n",
    "          validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model using sample weights\n",
    "loss, accuracy = model.evaluate(X_val, y_val, sample_weight=weight_val)\n",
    "print(f\"Weighted Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44487cc4-4e0f-4f11-932d-497b329b303d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y_val)\n",
    "true_labels = np.argmax(y_val, axis=1) + 1  # Convert from one-hot to 1-based class labels\n",
    "print(true_labels[true_labels==5].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978584f-bb7d-48bb-8c7c-a85f96a11368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predictions (softmax outputs)\n",
    "y_pred = model.predict(X_val)  # Shape: (num_samples, 5)\n",
    "\n",
    "# Convert one-hot encoded y_val back to integer labels (1-5)\n",
    "true_labels = np.argmax(y_val, axis=1) + 1  # Converts from one-hot to 1-5 based labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbbbc0-46c8-4020-9781-e763f953ffd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in labels_dict:  # Loop over 5 classes\n",
    "    plt.hist(y_pred[true_labels == labels_dict[key]], bins=50, \n",
    "             alpha=0.6, label=key, \n",
    "             density=True)\n",
    "\n",
    "plt.xlabel(\"Softmax Output\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Neural Network Softmax Outputs per Class\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079f304-3898-4c7a-b73b-2a2e63e6f44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in labels_dict:  # Loop over 5 classes\n",
    "    plt.hist(y_pred[true_labels == labels_dict[key]], bins=50, \n",
    "             alpha=0.6, label=key, \n",
    "             density=True)\n",
    "\n",
    "plt.xlabel(\"Softmax Output\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Neural Network Softmax Outputs per Class\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96188779-c8d9-4415-88ca-b654948c8f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c60a35-092b-4ea9-a7e1-ab9bc5dc6aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c14ff-b62a-41af-af3e-17f7eb52a46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2369320-ea2c-49ac-bf54-9bd0bd4e03d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7cb969-076b-40b1-9e68-be116781acf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b9643-02af-4ce7-a465-3d10a460ec19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
