{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, pathlib, importlib\n",
    "sys.path.append('../')\n",
    "\n",
    "# Load the package and modules for training and plotting\n",
    "import nsbi_common_utils\n",
    "from nsbi_common_utils import plotting, training\n",
    "from nsbi_common_utils.training import TrainEvaluate_NN, TrainEvaluatePreselNN\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import mplhep as hep\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "from utils import preselection_using_score, calculate_preselection_observable\n",
    "\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "hep.style.use(hep.style.ATLAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"config_new.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# path prefix for saving cached data used between modules\n",
    "path_prefix = config['path_prefix']\n",
    "path_saved_data = config['path_saved_data']\n",
    "path_preselection_NN_model = config['path_preselection_NN_model']\n",
    "\n",
    "# Path for saving interemdiate objects, like NN predictions\n",
    "saved_data = f'{path_prefix}{path_saved_data}'\n",
    "\n",
    "# Path for saving/loading preselection NN model\n",
    "path_to_saved_presel_model = f'{path_prefix}{path_preselection_NN_model}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the dictionary of labels to processes\n",
    "labels_dict = config[\"labels_dict\"]\n",
    "\n",
    "# Signal processes in the model\n",
    "signal_processes = config[\"signal_processes\"]\n",
    "\n",
    "# Background processes in the model\n",
    "background_processes = config[\"background_processes\"]\n",
    "\n",
    "print(signal_processes)\n",
    "print(background_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# If the preselection NN has already been trained and saved, load from the saved model\n",
    "USE_SAVED_MODEL_PRESEL = True\n",
    "\n",
    "# If the preselection NN has already been trained and evaluated, load the numpy array of predictions\n",
    "USE_SAVED_PRESEL_PREDICTIONS = True\n",
    "\n",
    "# Input features for training\n",
    "features = config[\"features\"]\n",
    "\n",
    "# Subset of the features to standardize before training\n",
    "features_scaling = config[\"features_scaling\"]\n",
    "\n",
    "# float factors multiplying the probabilities in the preselection score observable \n",
    "pre_factor_preselection_score = config[\"pre_factor_preselection_score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the nominal dataset saved from the pre-processing notebook\n",
    "\n",
    "path_to_nominal_dataframe = config[\"path_to_nominal_dataframe\"]\n",
    "dataset_incl_nominal = pd.read_hdf(path_to_nominal_dataframe, key=\"dataset\", mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the MC/data weights and training labels identifying different processes\n",
    "weights         = dataset_incl_nominal[\"weights\"].to_numpy()\n",
    "train_labels    = dataset_incl_nominal[\"train_labels\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Normalizing the training weights - only discriminating shapes\n",
    "weights_normed  = weights.copy()\n",
    "\n",
    "for key in labels_dict:\n",
    "\n",
    "    weights_normed[train_labels==labels_dict[key]] /= weights[train_labels==labels_dict[key]].sum()\n",
    "\n",
    "dataset_incl_nominal['weights_normed'] = weights_normed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Preselection NN\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choice of reference sample**\n",
    "\n",
    "The density ratios need to be trained on phase space regions with support for the reference hypothesis $p_{ref}(x) > 0$.\n",
    "\n",
    "To ensure this, we make a selection that selects events in the phase space regions with $p_{ref}(x) > 0$, or $p_c(x) \\gg p_{ref}(x)$, and only perform the NSBI fit in this selected analysis region. **A natural choice for the reference hypothesis is then the signal-rich hypotheses**. This is referred to in the ATLAS publications as the Search-Oriented Mixture Models approach: \n",
    "\n",
    "$$p_{ref}(x) = \\frac{1}{\\sum_S \\nu_S} \\sum_S \\frac{d\\sigma_S}{dx} = \\frac{1}{\\nu_{H \\to \\tau\\tau}} \\frac{d\\sigma_{H \\to \\tau\\tau}}{dx}$$\n",
    "\n",
    "where the sum runs over all signal hypothesis in the model and the second equality is due to the sole signal hypothesis in our toy model, $pp \\to {t\\bar{t}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# What are the signal processes in the user-provided model?\n",
    "print(signal_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The reference hypothesis is chosen as the sum of signal hypothesis\n",
    "ref_processes = config[\"ref_processes\"]\n",
    "print(ref_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting out regions with $p_{ref}\\sim 0$**\n",
    "\n",
    "A multi-class classification NN, with softmax output, is trained to output a score:\n",
    "\n",
    "$$ \\text{NN}_\\text{presel} = \\log \\left[\\frac{\\sum_S P_S (x)}{\\sum_B P_B(x)} \\right]$$\n",
    "\n",
    "where $P_c$ are the probability scores outputted from the softmax layer of the trained NN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['nsbi_common_utils.training'])\n",
    "from nsbi_common_utils.training import TrainEvaluatePreselNN\n",
    "\n",
    "num_classes = len(labels_dict)\n",
    "\n",
    "preselectionTraining = TrainEvaluatePreselNN(dataset_incl_nominal, \n",
    "                                            num_classes, \n",
    "                                            features, \n",
    "                                            features_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if USE_SAVED_PRESEL_PREDICTIONS:\n",
    "\n",
    "    pred_NN_incl = np.load(f\"{saved_data}pred_NN_incl.npy\")\n",
    "    presel_score = calculate_preselection_observable(pred_NN_incl, labels_dict, signal_processes, background_processes, pre_factor_dict = pre_factor_preselection_score)\n",
    "\n",
    "else:\n",
    "    if not USE_SAVED_MODEL_PRESEL:\n",
    "        preselectionTraining.train(test_size=0.2, \n",
    "                                   random_state=42, \n",
    "                                   path_to_save=path_to_saved_presel_model,\n",
    "                                  batch_size=4096,\n",
    "                                  epochs=50, learning_rate=0.1)\n",
    "    \n",
    "    else:\n",
    "        preselectionTraining.get_trained_model(path_to_saved_presel_model)\n",
    "\n",
    "    # Get predictions (softmax outputs)\n",
    "    pred_NN_incl = preselectionTraining.predict(dataset_incl_nominal)\n",
    "\n",
    "    presel_score = calculate_preselection_observable(pred_NN_incl, labels_dict, signal_processes, background_processes, pre_factor_dict = pre_factor_preselection_score)\n",
    "\n",
    "    np.save(f\"{saved_data}presel_score.npy\", presel_score)\n",
    "    np.save(f\"{saved_data}pred_NN_incl.npy\", pred_NN_incl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "min_pred = np.amin(presel_score)\n",
    "max_pred = np.amax(presel_score)\n",
    "\n",
    "bins = np.linspace(min_pred, max_pred, num=50)\n",
    "\n",
    "hist_NN_output = {}\n",
    "hist_NN_output_errs = {}\n",
    "\n",
    "for key in labels_dict: \n",
    "    hist_NN_output[key], _ = np.histogram(presel_score[train_labels==labels_dict[key]], \n",
    "                                          weights = weights[train_labels==labels_dict[key]], bins=bins)\n",
    "    \n",
    "    hist_NN_output_errs[key], _ = np.histogram(presel_score[train_labels==labels_dict[key]], \n",
    "                                          weights = weights[train_labels==labels_dict[key]]**2, bins=bins)\n",
    "\n",
    "\n",
    "for key in labels_dict:  \n",
    "    hep.histplot(hist_NN_output[key], bins=bins, \n",
    "             alpha=0.6, label=key, \n",
    "             density=True, linewidth=2.0, yerr = np.sqrt(hist_NN_output_errs[key]))\n",
    "\n",
    "plt.xlabel(\"Preselection Score\", size=18)\n",
    "plt.ylabel(\"Density\", size=18)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the cut\n",
    "===\n",
    "\n",
    "Make a selection cut for regions with $p_{ref} \\gg 0$ for performing the NSBI analysis. The remaining events - which by definition are background-dominated - can be used as a **Control Region** for data-driven background estimation, pre-unblinding validations, etc. \n",
    "\n",
    "Moreover, the preselections act as a tuning know for the tradeoff in selecting as many signal events as possible to go into the **Signal Region** (increasing sensitivity) and the feasibility of training accurate and precise NNs over a large phase space (need bigger models and more statistics). **The preselections can also weed out phase space regions with low background statistics to avoid poorly modelled regions.** \n",
    "\n",
    "Heres a first cut that you can optimize as much as you like to get the desired final results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Play around with these selections - decrease if estimators are unbiased but need more sensitivity and increase if the model is biased to reduce complexity\n",
    "preselection_cuts = {'upper': 4.5, 'lower': -1.}\n",
    "np.save(f\"{saved_data}preselection_cuts.npy\", preselection_cuts)\n",
    "\n",
    "for key in labels_dict:  \n",
    "    hep.histplot(hist_NN_output[key], bins = bins, \n",
    "             alpha = 0.6, label = key, \n",
    "             density = True, linewidth = 2.0, \n",
    "                 yerr = np.sqrt(hist_NN_output_errs[key]))\n",
    "\n",
    "plt.xlabel(\"Preselection Score\", size=18)\n",
    "\n",
    "for key in preselection_cuts:\n",
    "    if preselection_cuts[key] != -999:\n",
    "        plt.axvline(preselection_cuts[key], ymax=0.9, linestyle='--', label=f'preselection cut {key} = {preselection_cuts[key]}')\n",
    "\n",
    "plt.ylabel(\"Density\", size=18)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for key in labels_dict:  \n",
    "    hep.histplot(hist_NN_output[key], bins = bins, \n",
    "             alpha = 0.6, label = key, \n",
    "             density = False, linewidth = 2.0, \n",
    "                 yerr = np.sqrt(hist_NN_output_errs[key]))\n",
    "\n",
    "plt.xlabel(\"Preselection Score\", size=18)\n",
    "\n",
    "for key in preselection_cuts:\n",
    "    if preselection_cuts[key] != -999:\n",
    "        plt.axvline(preselection_cuts[key], ymax=0.9, linestyle='--', label=f'preselection cut {key} = {preselection_cuts[key]}')\n",
    "\n",
    "plt.ylabel(\"Density\", size=18)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal and Control Regions\n",
    "===\n",
    "\n",
    "The high signal over background phase space towards the right of the preselection cut shown above will be categorized as the **Signal Region** where the NSBI analysis is performed.\n",
    "\n",
    "The low signal phase space towards the left will be used as a **Control Region**, with typical uses such as background estimation, pre-unblinding data-MC checks, etc. In this phase space, we will use a binned summary observable like in any traditional analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import preselection_using_score\n",
    "\n",
    "\n",
    "dataset_incl_nominal['presel_score'] = presel_score\n",
    "\n",
    "channel_selections = {'CR': {'observable': 'presel_score', \n",
    "                             'lower_presel': -999, \n",
    "                             'upper_presel': preselection_cuts.get('lower'), \n",
    "                             'num_bins': 4},\n",
    "                      \n",
    "                      'SR_binned': {'observable': 'presel_score', \n",
    "                                    'lower_presel': preselection_cuts.get('upper'), \n",
    "                                    'upper_presel': -999,\n",
    "                                    'num_bins': 1},\n",
    "                      \n",
    "                      'SR': {'observable': None, \n",
    "                             'upper_presel': preselection_cuts.get('upper'), \n",
    "                             'lower_presel': preselection_cuts.get('lower')}}\n",
    "\n",
    "dataset_channels = preselection_using_score(dataset_incl_nominal, channel_selections)\n",
    "\n",
    "del dataset_incl_nominal"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
