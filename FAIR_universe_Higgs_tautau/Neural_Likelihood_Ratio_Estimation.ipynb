{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85242300-60a2-467d-9f11-8af27bdc312a",
   "metadata": {},
   "source": [
    "Train Neural Networks to estimate Likelihood Ratios\n",
    "===\n",
    "\n",
    "In this notebook we will setup the neural networks that train unbiased and low-variance density ratios to be then used for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a769dae-97a2-44f9-bc81-e4c74ff7bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib, importlib\n",
    "sys.path.append('../')\n",
    "\n",
    "# Load the package and modules for training and plotting\n",
    "import nsbi_common_utils\n",
    "from nsbi_common_utils import plotting, training, datasets, configuration\n",
    "from nsbi_common_utils.training import TrainEvaluate_NN, TrainEvaluatePreselNN\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import mplhep as hep\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "from utils import preselection_using_score, calculate_preselection_observable\n",
    "\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "hep.style.use(hep.style.ATLAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['nsbi_common_utils.configuration'])\n",
    "from nsbi_common_utils import configuration\n",
    "\n",
    "config = nsbi_common_utils.configuration.ConfigManager(file_path_string = './config.yml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec064a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features for training\n",
    "features, features_scaling = config.get_training_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d065f-de1e-4c32-8adb-6879ff3c412f",
   "metadata": {},
   "source": [
    "Statistical Model\n",
    "-\n",
    "\n",
    "The statistical model we have is: \n",
    "\n",
    "$$p(x|\\mu, \\alpha) = \\frac{1}{\\nu(\\mu, \\alpha)} \\sum_c f_c(\\mu) \\cdot \\nu_c(\\alpha) \\cdot p_c\\left(x|\\alpha\\right)$$\n",
    "\n",
    "where $c$ stands for the various physics channels that contribute to the final state $x$, **$\\mu$ is the signal-strength parameter and $\\alpha$ is the vector of nuisance parameters associated with the various systematic uncertainties in the model**. Note that we are assuming that the parameter $\\mu$-dependence is known analytically and that we have simulation models for each of the channels $p_c(x)$. \n",
    "\n",
    "Setting up the test\n",
    "-\n",
    "\n",
    "The objective is to build the test statistic for composite hypothesis testing:\n",
    "\n",
    "$$t_\\mu = -2 \\cdot \\frac{\\text{Pois}(\\mathcal{N}_\\text{evts}|\\mu, \\hat{\\hat{\\alpha}})}{\\text{Pois}(\\mathcal{N}_\\text{evts}|\\hat{\\mu}, \\hat{\\alpha})} -2 \\cdot \\sum_i^{\\mathcal{N}_\\text{evts}} w_i \\times \\log \\frac{p(x_i|\\mu, \\hat{\\hat{\\alpha}})}{p(x_i|\\hat{\\mu}, \\hat{\\alpha})} + \\sum_m^{N_\\text{systs}} \\alpha_m^2$$\n",
    "\n",
    "A direct approach would then be to model the probability density $p(x|\\mu, \\alpha)$ using NNs. But that is a more difficult task than training probability density ratios. We use a simple trick:\n",
    "\n",
    "$$t_\\mu = -2 \\cdot \\frac{\\text{Pois}(\\mathcal{N}_\\text{evts}|\\mu, \\hat{\\hat{\\alpha}})}{\\text{Pois}(\\mathcal{N}_\\text{evts}|\\hat{\\mu}, \\hat{\\alpha})} -2 \\cdot \\sum_i^{\\mathcal{N}_\\text{evts}} w_i \\times \\log \\frac{p(x_i|\\mu, \\hat{\\hat{\\alpha}})/p_{ref}(x)}{p(x_i|\\hat{\\mu}, \\hat{\\alpha})/p_{ref}(x)} + \\sum_m^{N_\\text{systs}} \\alpha_m^2$$\n",
    "\n",
    "\n",
    "to turn this into a density ratio estimation problem:\n",
    "\n",
    "$$\\frac{p(x|\\mu, \\alpha)}{p_{ref}(x)} = \\frac{1}{\\nu(\\mu, \\alpha)} \\sum_c f_c(\\mu) \\cdot \\nu_c(\\alpha) \\cdot \\frac{p_c\\left(x|\\alpha\\right)}{p_{ref}(x)}$$\n",
    "\n",
    "Factorizing out the $\\alpha$-dependence in the density ratios:\n",
    "\n",
    "$$\\frac{p(x|\\mu, \\alpha)}{p_{ref}(x)} = \\frac{1}{\\sum_c G_c(\\alpha) \\cdot f_c(\\mu) \\cdot \\nu_c} \\sum_c f_c(\\mu) \\cdot G_c(\\alpha)\\cdot \\nu_c \\cdot g_c(x|\\alpha) \\cdot \\frac{p_c\\left(x\\right)}{p_{ref}(x)}$$\n",
    "\n",
    "Factorized approach:\n",
    "--\n",
    "\n",
    "Instead of training NNs parameterized on $\\mu$ and $\\alpha$, we use analytical parameterizations to simplify the problem to training only parameter-independent density ratios.\n",
    "\n",
    "Let's set this up for the toy $H\\to \\tau\\tau$ model we are using. **Neglecting the nuisance parameter dependence ($G_c(\\alpha) = g_c(x|\\alpha) = 1$), the probability model can be written as** \n",
    "\n",
    "$$\\sum_c f_c(\\mu) \\cdot \\nu_c(\\mu) \\cdot \\frac{p_c\\left(x\\right)}{p_{ref}(x)} = \\mu \\cdot \\nu_{H \\to \\tau\\tau}(\\mu) \\cdot  \\frac{p_{H \\to \\tau\\tau}\\left(x\\right)}{p_{ref}(x)} + \\nu_{t\\bar{t}}(\\mu) \\cdot  \\frac{p_{t\\bar{t}}\\left(x\\right)}{p_{ref}(x)} + \\nu_{Z \\to \\tau\\tau}(\\mu) \\cdot  \\frac{p_{Z \\to \\tau\\tau}\\left(x\\right)}{p_{ref}(x)}$$\n",
    "\n",
    "In the NSBI approach presented in this tutorial, the density ratio estimation as a function of the various nuisance parameters is factorized from the nominal density ratio estimation, and will be done in the  `Systematic_Uncertainty_Estimation.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14831cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['nsbi_common_utils.datasets'])\n",
    "from nsbi_common_utils import datasets\n",
    "\n",
    "branches_to_load = features + ['presel_score']\n",
    "\n",
    "Datasets = nsbi_common_utils.datasets.datasets(config_path = './config.yml',\n",
    "                                                branches_to_load =  branches_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_incl_dict      = Datasets.load_datasets_from_config(load_systematics = False)\n",
    "\n",
    "dataset_incl_nominal   = dataset_incl_dict[\"Nominal\"].copy()\n",
    "\n",
    "dataset_SR_nominal     = Datasets.filter_region_dataset(dataset_incl_nominal,\n",
    "                                                       region = \"SR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d325c1-c6ba-46f7-bdcb-fb9f4c120986",
   "metadata": {},
   "source": [
    "Density ratio training\n",
    "===\n",
    "\n",
    "Now we train the NNs for $\\frac{p_c}{p_{ref}}(x)$ density ratios to build the full model. Our choice of reference hypothesis, motivated by the search-oriented mixture model described above, would be: \n",
    "\n",
    "$$p_{ref}(x) = p_{H \\to \\tau\\tau}(x) = \\frac{1}{\\nu_{H \\to \\tau\\tau}} \\frac{d\\sigma_{H \\to \\tau\\tau}}{dx} $$\n",
    "\n",
    "gives the POI $\\mu-$parameterized model:\n",
    "\n",
    "$$\\sum_c \\left[f_c(\\mu) \\cdot \\frac{p_c\\left(x\\right)}{p_{ref}(x)} \\right]= \\mu + \\frac{p_{t\\bar{t}}\\left(x\\right)}{p_{H \\to \\tau\\tau}(x)} + \\frac{p_{Z \\to \\tau\\tau}\\left(x\\right)}{p_{H \\to \\tau\\tau}(x)}$$\n",
    "\n",
    "But this has shown to cause numerical issues, since the ratio corresponding to the $\\mu$-scaling signal term is exactly 1. In the absence of additional signal production mechansims, we instead use:\n",
    "\n",
    "$$p_{ref}(x) = \\frac{1}{\\nu_{H \\to \\tau\\tau} + \\nu_{t\\bar{t}}} \\left[\\frac{d\\sigma_{H \\to \\tau\\tau}}{dx} + \\frac{d\\sigma_{t\\bar{t}}}{dx} \\right]$$\n",
    "\n",
    "giving a more numerically stable model:\n",
    "\n",
    "$$\\sum_c \\left[f_c(\\mu) \\cdot \\frac{p_c\\left(x\\right)}{p_{ref}(x)} \\right]= \\mu \\cdot \\frac{p_{H \\to \\tau\\tau}\\left(x\\right)}{p_{ref}(x)} + \\frac{p_{t\\bar{t}}\\left(x\\right)}{p_{ref}(x)} + \\frac{p_{Z \\to \\tau\\tau}\\left(x\\right)}{p_{ref}(x)}$$\n",
    "\n",
    "The task of estimating the $\\mu-$parameterized density ratio is thus reduced to estimating three $\\mu-$independent density ratios $\\frac{p_{H \\to \\tau\\tau}}{p_{ref}}(x)$, $\\frac{p_{t\\bar{t}}}{p_{ref}}(x)$ and $\\frac{p_{Z \\to \\tau\\tau}}{p_{ref}}(x)$ mixed together with an analytical parameterization (hence the name mixture model).\n",
    "\n",
    "**NB**: This is the same as the *morphing-aware* approach of density ratio estimation in MadMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a4b5b-fa08-4c7c-a240-7353809b67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processes that make up the mixture model formula for density ratio estimation\n",
    "\n",
    "PATH_TO_SAVED_DATA = config.config[\"datasets_dir\"]\n",
    "TRAINING_OUTPUT_PATH = f'{PATH_TO_SAVED_DATA}output_training_nominal/'\n",
    "\n",
    "# Signal processes in the model\n",
    "basis_processes = config.get_basis_samples()\n",
    "print(basis_processes)\n",
    "\n",
    "ref_processes = config.get_reference_samples()\n",
    "print(ref_processes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966e691-14bb-4c04-8de3-880e5ccea5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['nsbi_common_utils.training'])\n",
    "from nsbi_common_utils.training import TrainEvaluate_NN\n",
    "\n",
    "ref_train_label_sample_dict = {**{ref: 0 for ref in ref_processes}}\n",
    "\n",
    "dataset_ref     = Datasets.merge_dataframe_dict_for_training(dataset_SR_nominal, \n",
    "                                                            ref_train_label_sample_dict, \n",
    "                                                            samples_to_merge = ref_processes)\n",
    "\n",
    "NN_training_mix_model = {}\n",
    "\n",
    "use_log_loss = False\n",
    "\n",
    "# DELETE_EXISTING_MODELS = True\n",
    "DELETE_EXISTING_MODELS = False\n",
    "\n",
    "path_to_ratios = {}\n",
    "path_to_figures = {}\n",
    "path_to_models = {}\n",
    "\n",
    "for process_type in basis_processes:\n",
    "\n",
    "    # Get the dictionary of labels to processes\n",
    "    _train_label_sample_dict = {process_type        : 1}\n",
    "\n",
    "    dataset_num     = Datasets.merge_dataframe_dict_for_training(dataset_SR_nominal, \n",
    "                                                                _train_label_sample_dict, \n",
    "                                                                samples_to_merge = [process_type])\n",
    "\n",
    "    # Build a training dataset for the training of p_<process_type>/p_<ref_processes> density ratio\n",
    "    dataset_mix_model = pd.concat([dataset_num, dataset_ref])\n",
    "\n",
    "    # Save paths\n",
    "    output_name                     = f'{process_type}'\n",
    "    output_dir                      = f'{TRAINING_OUTPUT_PATH}general_output_{process_type}'\n",
    "    path_to_ratios[process_type]    = f'{TRAINING_OUTPUT_PATH}output_ratios_{process_type}/'\n",
    "    path_to_figures[process_type]   = f'{TRAINING_OUTPUT_PATH}output_figures_{process_type}/'\n",
    "    path_to_models[process_type]    = f'{TRAINING_OUTPUT_PATH}output_model_params_{process_type}/'\n",
    "\n",
    "    NN_training_mix_model[process_type] = TrainEvaluate_NN(dataset_mix_model, \n",
    "                                                           dataset_mix_model['weights_normed'].to_numpy(),\n",
    "                                                           dataset_mix_model['train_labels'].to_numpy(),\n",
    "                                                           features, \n",
    "                                                           features_scaling, \n",
    "                                                           [process_type, 'ref'], \n",
    "                                                            output_dir, output_name, \n",
    "                                                            path_to_figures=path_to_figures[process_type],\n",
    "                                                            path_to_ratios=path_to_ratios[process_type], \n",
    "                                                            path_to_models=path_to_models[process_type],\n",
    "                                                            use_log_loss = use_log_loss,\n",
    "                                                            delete_existing_models = DELETE_EXISTING_MODELS)\n",
    "\n",
    "\n",
    "    del dataset_mix_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ecb1e9-7125-4b9b-ba07-ff625f6bfd83",
   "metadata": {},
   "source": [
    "Neural Network architecture\n",
    "===\n",
    "\n",
    "Here we will start with a multi-layer perceptron (simple feed-forward Neural Network with fully-connected layers). This can be supplemented with more complex architectures like transformers. \n",
    "\n",
    "After detailed tuning during the off-shell Higgs boson analysis effort, we found that a network with very wide layers ($\\geq 1000$) and a depth of less than 10 works best - alongside batch size of a few hundreds and a gradually declining learning rate that starts with a value just large enough to not blow up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f3dd7-eb37-4440-a316-090ce6136c43",
   "metadata": {},
   "source": [
    "Starting with the training and validation of the various density ratio:\n",
    "\n",
    "$$\\frac{p_{H\\to\\tau\\tau}(x)}{p_\\text{ref}(x)}, \\frac{p_{t\\bar{t}}(x)}{p_\\text{ref}(x)}, \\frac{p_{Z \\to \\tau\\tau}(x)}{p_\\text{ref}(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973aa36-189e-436d-9b18-add588b946aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de59bde-2332-4488-be38-62fae1ed4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can either train your own NNs (in which case set this to False) or you can load the pre-trained NNs. \n",
    "# Training NNs without GPUs can be very slow, for the kind of NNs we need for unbiased estimation of density ratios\n",
    "USE_SAVED_MODELS = True\n",
    "\n",
    "# Define unique settings for each process type\n",
    "doCalibration = False\n",
    "RECALIBRATE = False\n",
    "num_bins_cal = 50\n",
    "scaling_type = 'MinMax'\n",
    "batch_size = 4096\n",
    "validation_split = 0.1\n",
    "holdout_split = 0.25\n",
    "\n",
    "num_epochs = 100\n",
    "callback_patience = 30\n",
    "\n",
    "num_layers = 4\n",
    "num_neurons_per_layer = 1000\n",
    "\n",
    "num_ensemble_members = 20\n",
    "\n",
    "verbose_level = 0\n",
    "\n",
    "PLOT_SCALED_FEATURES = False\n",
    "\n",
    "training_settings = {\n",
    "\n",
    "    'htautau': {\n",
    "        'hidden_layers':        num_layers,\n",
    "        'neurons':              num_neurons_per_layer,\n",
    "        'number_of_epochs':     num_epochs,\n",
    "        'batch_size':           batch_size,\n",
    "        'learning_rate':        0.1,\n",
    "        'scalerType':           scaling_type,\n",
    "        'calibration':          doCalibration,\n",
    "        'num_bins_cal':         num_bins_cal,\n",
    "        'callback':             True,\n",
    "        'callback_patience':    callback_patience,\n",
    "        'callback_factor':      0.01,\n",
    "        'validation_split':     validation_split,\n",
    "        'holdout_split':        holdout_split,\n",
    "        'verbose':              verbose_level,\n",
    "        'plot_scaled_features': PLOT_SCALED_FEATURES,\n",
    "        'load_trained_models':  USE_SAVED_MODELS,\n",
    "        'recalibrate_output'   : RECALIBRATE,\n",
    "        'num_ensemble_members': num_ensemble_members\n",
    "    },\n",
    "    \n",
    "    'ttbar': {\n",
    "        'hidden_layers':        num_layers,\n",
    "        'neurons':              num_neurons_per_layer,\n",
    "        'number_of_epochs':     num_epochs,\n",
    "        'batch_size':           batch_size,\n",
    "        'learning_rate':        0.1,\n",
    "        'scalerType':           scaling_type,\n",
    "        'calibration':          doCalibration,\n",
    "        'num_bins_cal':         num_bins_cal,\n",
    "        'callback':             True,\n",
    "        'callback_patience':    callback_patience,\n",
    "        'callback_factor':      0.01,\n",
    "        'validation_split':     validation_split,\n",
    "        'holdout_split':        holdout_split,\n",
    "        'verbose':              verbose_level,\n",
    "        'plot_scaled_features': PLOT_SCALED_FEATURES,\n",
    "        'load_trained_models':  USE_SAVED_MODELS,\n",
    "        'recalibrate_output'   : RECALIBRATE,\n",
    "        'num_ensemble_members': num_ensemble_members\n",
    "    },\n",
    "    \n",
    "    'ztautau': {\n",
    "        'hidden_layers':        num_layers,\n",
    "        'neurons':              num_neurons_per_layer,\n",
    "        'number_of_epochs':     num_epochs,\n",
    "        'batch_size':           batch_size,\n",
    "        'learning_rate':        0.1,\n",
    "        'scalerType':           scaling_type,\n",
    "        'calibration':          doCalibration,\n",
    "        'num_bins_cal':         num_bins_cal,\n",
    "        'callback':             True,\n",
    "        'callback_patience':    callback_patience,\n",
    "        'callback_factor':      0.01,\n",
    "        'validation_split':     validation_split,\n",
    "        'holdout_split':        holdout_split,\n",
    "        'verbose':              verbose_level,\n",
    "        'plot_scaled_features': PLOT_SCALED_FEATURES,\n",
    "        'load_trained_models':  USE_SAVED_MODELS,\n",
    "        'recalibrate_output'   : RECALIBRATE,\n",
    "        'num_ensemble_members': num_ensemble_members\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de9378-0c65-4425-b21b-acef0d2a344f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the density ratio NN\n",
    "for process_type in basis_processes:\n",
    "    \n",
    "    settings = training_settings[process_type]\n",
    "\n",
    "    NN_training_mix_model[process_type].train_ensemble(**settings)\n",
    "    NN_training_mix_model[process_type].test_normalization()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62e325-9a46-4d80-b262-089a8dbc8ece",
   "metadata": {},
   "source": [
    "Check for overtraining by comparing the NN output distributions between training and holdout datasets. The holdout dataset is the subset of events not used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for process_type in basis_processes:\n",
    "    NN_training_mix_model[process_type].make_overfit_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed7896-6ca2-4c24-a816-d8e2a645b466",
   "metadata": {},
   "source": [
    "Diagnostic Checks\n",
    "===\n",
    "\n",
    "While traditionally, a NN observable is judged on the basis of its accuracy - for NSBI we are interested in the quality of the density ratios more than the discrimination power. The latter comes from the perfect modelling of the multi-dimensional likelihood ratios.\n",
    "\n",
    "To ensure correct modelling, we run two main checks on the training:\n",
    "\n",
    "- **Calibration closure test**\n",
    "\n",
    "  The NNs are trained using the binary cross-entropy loss, which under ideal conditions leads to the NN converging to the score function:\n",
    "\n",
    "  $$\\hat{s}_\\text{pred} = \\frac{p_\\text{ref}(x)}{p_\\text{ref}(x)+p_\\text{c}(x)}$$\n",
    "\n",
    "  that can be converted into the probability ratio we desire (likelihood ratio trick):\n",
    "\n",
    "  $$\\frac{p_\\text{c}(x)}{p_\\text{ref}(x)} = \\frac{\\hat{s}_\\text{pred}(x)}{1-\\hat{s}_\\text{pred}(x)}$$\n",
    "\n",
    "  For the NNs to be well-calibrated, we use the Monte Carlo samples to verify the equality:\n",
    "\n",
    "\n",
    "  $$\\left[\\frac{p_c(x)}{p_c(x)+p_{ref}(x)}\\right]_\\text{NN} \\sim \\left[\\frac{\\mathcal{N}_c^{I(x|\\hat{s}_\\text{pred})}}{\\mathcal{N}_c^{I(x|\\hat{s}_\\text{pred})}+\\mathcal{N}_\\text{ref}^{I(x|\\hat{s}_\\text{pred})}}\\right]_\\text{MC}$$\n",
    "\n",
    "  where we bin the events from $p_c$ and $p_\\text{ref}$ MC samples, denoted by $\\mathcal{N}_c^{I(x|\\hat{s}_\\text{pred})}$ and $\\mathcal{N}_\\text{ref}^{I(x|\\hat{s}_\\text{pred})}$ respectively where $I(x|\\hat{s}_\\text{pred})$ returns the index of the $\\hat{s}_\\text{pred}$ bin in which an event $x$ falls.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aba043",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins_cal = 50\n",
    "\n",
    "for process_type in basis_processes:\n",
    "    NN_training_mix_model[process_type].make_calib_plots(nbins=num_bins_cal, observable='score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2a7a2-f890-4ed3-8fc2-764ab989e494",
   "metadata": {},
   "source": [
    "## Density ratio reweighting closure tests\n",
    "  \n",
    "  Despite having a well-calibrated output and thus a robust probabilistic interpretation, the trained density ratios might not capture the full multi-dimensional event information $x$. In other words, the NNs might still be biased estimators of the optimal score function, as defined in the CARL paper (link).\n",
    "\n",
    "  The next diagnostic involves verifying the following equality using 1D projections of $x$:\n",
    "\n",
    "  $$\\frac{p_c(x)}{p_{ref}(x)} \\times p_{ref}(x) \\sim p_c(x)$$\n",
    "\n",
    "  We can do this one-by-one for all the observables used to model the density ratios, and also possibly the observables not used directly in the training but can still be well-estimated due to the NN learning the right physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad88322",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_plot=['log_DER_pt_h'] # The 1D variable for reweighting closure\n",
    "yscale_type='log'\n",
    "num_bins_plotting=21\n",
    "\n",
    "for process_type in basis_processes:\n",
    "\n",
    "    NN_training_mix_model[process_type].make_reweighted_plots(variables_to_plot, yscale_type, num_bins_plotting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc68bf7-a294-47ba-b1d8-38703bdc9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combined_SR = Datasets.merge_dataframe_dict_for_training(dataset_SR_nominal, None, \n",
    "                                                                samples_to_merge = [\"htautau\", \"ztautau\", \"ttbar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891bd136-e59f-49a5-a853-073482a8e33a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_saved_ratios = {}\n",
    "\n",
    "for process_type in basis_processes:\n",
    "    # Evaluate the density ratios p_c/p_ref for the full dataset and save for the inference step\n",
    "    path_to_saved_ratios[process_type] = NN_training_mix_model[process_type].evaluate_and_save_ratios(dataset_combined_SR, \n",
    "                                                                                                     aggregation_type = 'median_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c18575-9da4-4c4a-b1fb-8289c6287b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_saved_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e61653-61ea-4666-9bdf-f7b3e8b99c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = f\"{PATH_TO_SAVED_DATA}/dataset_Asimov.root\"\n",
    "nsbi_common_utils.datasets.save_dataframe_as_root(dataset_combined_SR, \n",
    "                                                  path_to_save = path_to_save,\n",
    "                                                  tree_name = \"nominal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938306e-0630-4a21-a8af-41c9b896300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = f\"{PATH_TO_SAVED_DATA}/asimov_weights.npy\"\n",
    "np.save(f\"{path_to_save}\", dataset_combined_SR[\"weights\"].to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa27cca-fecf-426a-a7c5-a77d82a4de2a",
   "metadata": {},
   "source": [
    "Systematic Uncertainty Modelling\n",
    "===\n",
    "\n",
    "So far we have left out the nuisance parameter piece of the parameterized density ratio decomposition shown before:\n",
    "\n",
    "$$g_c(x|\\alpha) = \\frac{p_c(x|\\alpha)}{p_c(x)}$$\n",
    "\n",
    "which will be estimated in the `Systematic_Uncertainty_Estimation.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d5999-ac49-4fdf-b2b2-4296b7e82bf4",
   "metadata": {},
   "source": [
    "Training an Ensemble of NNs\n",
    "===\n",
    "\n",
    "In a realistic analysis, a large ensemble of NNs are trained to estimate density ratios, for *each* of the channels. **This reduces not only the variance but has also been found to reduce biases in the final fits.**\n",
    "\n",
    "In the off-shell Higgs boson analysis, we used k-fold cross-validation, with k=10, to train an ensemble $N_{ens}\\sim 250$ of NNs per density ratio $p_c(x)/p_{ref}(x)$. The density ratios used in the final fit are always evaluated on the holdout dataset and by using each of the folds as holdout once, the entire model for $p_c(x)/p_{ref}(x)$ consists of $N_{ens}\\times k \\sim 2500$ NNs. Each NN in the ensemble is trained by resampling data without replacement from the k-1 folds used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d57a3f-b14e-4691-adad-1e7f1f344f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33132a99-776c-4170-85f7-27ef352707c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pixi: nsbi-env-gpu-new-higgs)",
   "language": "python",
   "name": "nsbi-env-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "pixi-kernel": {
   "environment": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
